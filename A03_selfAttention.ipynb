{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment-03\n",
        "​\n",
        "# BM23MTECH11006\n",
        "​\n",
        "# PITHANI TEJA VENKATA RAMANA KUMAR\n"
      ],
      "metadata": {
        "id": "Bg5PsoAnC7XR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "W6EAmzQBC6RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:37:38.057429Z",
          "iopub.execute_input": "2024-03-18T18:37:38.058267Z",
          "iopub.status.idle": "2024-03-18T18:37:45.219836Z",
          "shell.execute_reply.started": "2024-03-18T18:37:38.058220Z",
          "shell.execute_reply": "2024-03-18T18:37:45.219079Z"
        },
        "trusted": true,
        "id": "mQizFP76C6RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "CFn_1sWOC6RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(32),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:37:45.221296Z",
          "iopub.execute_input": "2024-03-18T18:37:45.221910Z",
          "iopub.status.idle": "2024-03-18T18:37:50.398778Z",
          "shell.execute_reply.started": "2024-03-18T18:37:45.221884Z",
          "shell.execute_reply": "2024-03-18T18:37:50.397987Z"
        },
        "trusted": true,
        "id": "BMJcThQpC6RL",
        "outputId": "66487831-d89b-4f3b-fe48-8962f8e220fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 170498071/170498071 [00:01<00:00, 100988651.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(trainset[0][0][1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:37:50.399794Z",
          "iopub.execute_input": "2024-03-18T18:37:50.400052Z",
          "iopub.status.idle": "2024-03-18T18:37:50.793570Z",
          "shell.execute_reply.started": "2024-03-18T18:37:50.400030Z",
          "shell.execute_reply": "2024-03-18T18:37:50.792617Z"
        },
        "trusted": true,
        "id": "10bBdLanC6RN",
        "outputId": "e39fb6de-5747-4a75-bcc6-c11fab13cf5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<matplotlib.image.AxesImage at 0x7cbf133e9db0>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Self-Attention for Object Recognition with CNNs: Implement a sample CNN with one or more self-attention layer(s) for performing object recognition over CIFAR-10 dataset. You have to implement the self-attention layer yourself and use it in the forward function defined by you. All other layers (fully connected, nonlinearity, conv layer, etc.) can be bulit-in implementations. The network can be a simpler one (e.g., it may have 1x Conv, 4x [Conv followed by SA], 1x Conv, and 1x GAP). Please refer to the reading material provided here or any other similar one. [10 Marks]\n"
      ],
      "metadata": {
        "id": "pDeBa0ovC6RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Self Attention Layer\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, width, height = x.size()\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, width*height).permute(0, 2, 1)  # B x (N) x C\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, width*height)  # B x C x (N)\n",
        "        energy = torch.bmm(proj_query, proj_key)  # batch matrix multiplication\n",
        "        attention = F.softmax(energy, dim=-1)  # normalize across keys\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, width*height)  # B x C x N\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, width, height)\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# Define the CNN model with Self Attention\n",
        "class SelfAttentionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SelfAttentionCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.sa1 = SelfAttention(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.sa2 = SelfAttention(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.sa3 = SelfAttention(128)\n",
        "        self.conv5 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(64, 10, 3, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.sa1(x)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.sa2(x)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.sa3(x)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:38:07.785470Z",
          "iopub.execute_input": "2024-03-18T18:38:07.785825Z",
          "iopub.status.idle": "2024-03-18T18:38:07.802163Z",
          "shell.execute_reply.started": "2024-03-18T18:38:07.785796Z",
          "shell.execute_reply": "2024-03-18T18:38:07.801140Z"
        },
        "trusted": true,
        "id": "omFabHDFC6RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "net = SelfAttentionCNN()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the GPU\n",
        "net.to(device)\n",
        "\n",
        "# Move the data to the GPU inside the training loop\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)  # Move input data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 100 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:38:49.121209Z",
          "iopub.execute_input": "2024-03-18T18:38:49.121902Z",
          "iopub.status.idle": "2024-03-18T18:45:04.944177Z",
          "shell.execute_reply.started": "2024-03-18T18:38:49.121873Z",
          "shell.execute_reply": "2024-03-18T18:45:04.943232Z"
        },
        "trusted": true,
        "id": "XgUBjxpwC6RO",
        "outputId": "f6fb3603-a128-478e-a813-faec3bfae21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[1,   200] loss: 2.186\n[1,   400] loss: 1.911\n[1,   600] loss: 1.750\n[2,   200] loss: 1.530\n[2,   400] loss: 1.462\n[2,   600] loss: 1.412\n[3,   200] loss: 1.280\n[3,   400] loss: 1.145\n[3,   600] loss: 1.047\n[4,   200] loss: 0.933\n[4,   400] loss: 0.911\n[4,   600] loss: 0.881\n[5,   200] loss: 0.790\n[5,   400] loss: 0.800\n[5,   600] loss: 0.801\n[6,   200] loss: 0.708\n[6,   400] loss: 0.713\n[6,   600] loss: 0.720\n[7,   200] loss: 0.648\n[7,   400] loss: 0.645\n[7,   600] loss: 0.657\n[8,   200] loss: 0.575\n[8,   400] loss: 0.585\n[8,   600] loss: 0.609\n[9,   200] loss: 0.519\n[9,   400] loss: 0.543\n[9,   600] loss: 0.537\n[10,   200] loss: 0.475\n[10,   400] loss: 0.493\n[10,   600] loss: 0.513\nFinished Training\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    device = next(model.parameters()).device  # Get the device where the model is located\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)  # Move input data to GPU\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:45:13.224250Z",
          "iopub.execute_input": "2024-03-18T18:45:13.225084Z",
          "iopub.status.idle": "2024-03-18T18:45:13.231189Z",
          "shell.execute_reply.started": "2024-03-18T18:45:13.225049Z",
          "shell.execute_reply": "2024-03-18T18:45:13.230252Z"
        },
        "trusted": true,
        "id": "t3a_nA5BC6RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "# Assuming you have already defined your model and dataloader\n",
        "test_accuracy = evaluate_accuracy(net, testloader)\n",
        "print(f'Test Accuracy: {test_accuracy:.2%}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:45:17.117628Z",
          "iopub.execute_input": "2024-03-18T18:45:17.118560Z",
          "iopub.status.idle": "2024-03-18T18:45:21.359944Z",
          "shell.execute_reply.started": "2024-03-18T18:45:17.118521Z",
          "shell.execute_reply": "2024-03-18T18:45:21.359021Z"
        },
        "trusted": true,
        "id": "AEyWMXYMC6RQ",
        "outputId": "509c719c-0215-4f06-ae94-695332dba850"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test Accuracy: 74.10%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Object Recognition with Vision Transformer: Implement and train an Encoder only Trans\u0002former (ViT-like) for the above object recognition task. In other words, implement multi-headed self-attention for the image classification (i.e., appending a < class > token to the image patches that are accepted as input tokens). Compare the performance of the two implementations (try to keep the number of parameters to be comparable and use the same amount of training and testing data). [10 Marks]"
      ],
      "metadata": {
        "id": "69qIlI10C6RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, embed_dim, num_heads, num_layers):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        assert image_size % patch_size == 0, \"image size must be divisible by patch size\"\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_embedding = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.class_embedding = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.transformer_encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embedding(x)  # B x C x H' x W'\n",
        "        x = x.flatten(2).transpose(1, 2)  # B x (H' * W') x C\n",
        "        x = torch.cat([self.class_embedding.expand(B, -1, -1), x], dim=1)  # B x (H' * W' + 1) x C\n",
        "        x = x + self.positional_encoding  # B x (H' * W' + 1) x C\n",
        "        for layer in self.transformer_encoder_layers:\n",
        "            x = layer(x)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(embed_dim, embed_dim * 4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(embed_dim * 4, embed_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x, _ = self.self_attn(x, x, x)\n",
        "        x = residual + self.dropout(x)\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = residual + self.dropout(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:45:52.861180Z",
          "iopub.execute_input": "2024-03-18T18:45:52.861804Z",
          "iopub.status.idle": "2024-03-18T18:45:52.876636Z",
          "shell.execute_reply.started": "2024-03-18T18:45:52.861773Z",
          "shell.execute_reply": "2024-03-18T18:45:52.875621Z"
        },
        "trusted": true,
        "id": "7p5UWjkGC6RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Vision Transformer model\n",
        "vit_model = VisionTransformer(image_size=32, patch_size=4, num_classes=10, embed_dim=64, num_heads=4, num_layers=4)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(vit_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    vit_model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:46:10.087811Z",
          "iopub.execute_input": "2024-03-18T18:46:10.088199Z",
          "iopub.status.idle": "2024-03-18T18:49:53.048483Z",
          "shell.execute_reply.started": "2024-03-18T18:46:10.088171Z",
          "shell.execute_reply": "2024-03-18T18:49:53.047513Z"
        },
        "trusted": true,
        "id": "MCG2HhzFC6RS",
        "outputId": "58d59fd2-aea2-4b1e-cddc-6b6d091c0dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/10], Step [100/782], Loss: 2.1624\nEpoch [1/10], Step [200/782], Loss: 1.9749\nEpoch [1/10], Step [300/782], Loss: 1.8804\nEpoch [1/10], Step [400/782], Loss: 1.7990\nEpoch [1/10], Step [500/782], Loss: 1.7369\nEpoch [1/10], Step [600/782], Loss: 1.6977\nEpoch [1/10], Step [700/782], Loss: 1.6706\nEpoch [2/10], Step [100/782], Loss: 1.5867\nEpoch [2/10], Step [200/782], Loss: 1.5729\nEpoch [2/10], Step [300/782], Loss: 1.5468\nEpoch [2/10], Step [400/782], Loss: 1.5129\nEpoch [2/10], Step [500/782], Loss: 1.4996\nEpoch [2/10], Step [600/782], Loss: 1.4965\nEpoch [2/10], Step [700/782], Loss: 1.4751\nEpoch [3/10], Step [100/782], Loss: 1.4568\nEpoch [3/10], Step [200/782], Loss: 1.4123\nEpoch [3/10], Step [300/782], Loss: 1.4427\nEpoch [3/10], Step [400/782], Loss: 1.4129\nEpoch [3/10], Step [500/782], Loss: 1.4189\nEpoch [3/10], Step [600/782], Loss: 1.3823\nEpoch [3/10], Step [700/782], Loss: 1.3453\nEpoch [4/10], Step [100/782], Loss: 1.3437\nEpoch [4/10], Step [200/782], Loss: 1.3500\nEpoch [4/10], Step [300/782], Loss: 1.3532\nEpoch [4/10], Step [400/782], Loss: 1.3375\nEpoch [4/10], Step [500/782], Loss: 1.3395\nEpoch [4/10], Step [600/782], Loss: 1.3192\nEpoch [4/10], Step [700/782], Loss: 1.2995\nEpoch [5/10], Step [100/782], Loss: 1.2987\nEpoch [5/10], Step [200/782], Loss: 1.2848\nEpoch [5/10], Step [300/782], Loss: 1.2988\nEpoch [5/10], Step [400/782], Loss: 1.2716\nEpoch [5/10], Step [500/782], Loss: 1.2825\nEpoch [5/10], Step [600/782], Loss: 1.2568\nEpoch [5/10], Step [700/782], Loss: 1.2727\nEpoch [6/10], Step [100/782], Loss: 1.2440\nEpoch [6/10], Step [200/782], Loss: 1.2370\nEpoch [6/10], Step [300/782], Loss: 1.2299\nEpoch [6/10], Step [400/782], Loss: 1.2335\nEpoch [6/10], Step [500/782], Loss: 1.2262\nEpoch [6/10], Step [600/782], Loss: 1.2517\nEpoch [6/10], Step [700/782], Loss: 1.2060\nEpoch [7/10], Step [100/782], Loss: 1.1768\nEpoch [7/10], Step [200/782], Loss: 1.2209\nEpoch [7/10], Step [300/782], Loss: 1.2092\nEpoch [7/10], Step [400/782], Loss: 1.1886\nEpoch [7/10], Step [500/782], Loss: 1.1973\nEpoch [7/10], Step [600/782], Loss: 1.1778\nEpoch [7/10], Step [700/782], Loss: 1.1990\nEpoch [8/10], Step [100/782], Loss: 1.1728\nEpoch [8/10], Step [200/782], Loss: 1.1929\nEpoch [8/10], Step [300/782], Loss: 1.1472\nEpoch [8/10], Step [400/782], Loss: 1.1785\nEpoch [8/10], Step [500/782], Loss: 1.1442\nEpoch [8/10], Step [600/782], Loss: 1.1614\nEpoch [8/10], Step [700/782], Loss: 1.1399\nEpoch [9/10], Step [100/782], Loss: 1.1368\nEpoch [9/10], Step [200/782], Loss: 1.1465\nEpoch [9/10], Step [300/782], Loss: 1.1194\nEpoch [9/10], Step [400/782], Loss: 1.1307\nEpoch [9/10], Step [500/782], Loss: 1.1263\nEpoch [9/10], Step [600/782], Loss: 1.1298\nEpoch [9/10], Step [700/782], Loss: 1.1211\nEpoch [10/10], Step [100/782], Loss: 1.0901\nEpoch [10/10], Step [200/782], Loss: 1.1011\nEpoch [10/10], Step [300/782], Loss: 1.1180\nEpoch [10/10], Step [400/782], Loss: 1.1015\nEpoch [10/10], Step [500/782], Loss: 1.1005\nEpoch [10/10], Step [600/782], Loss: 1.0956\nEpoch [10/10], Step [700/782], Loss: 1.0874\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "vit_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = vit_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy of the Vision Transformer model on the 10000 test images: {accuracy:.2%}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-18T18:49:53.050009Z",
          "iopub.execute_input": "2024-03-18T18:49:53.050296Z",
          "iopub.status.idle": "2024-03-18T18:49:56.352723Z",
          "shell.execute_reply.started": "2024-03-18T18:49:53.050273Z",
          "shell.execute_reply": "2024-03-18T18:49:56.351608Z"
        },
        "trusted": true,
        "id": "8JCVo1lGC6RS",
        "outputId": "53d08128-afeb-49f3-b99e-2a498f4aa788"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy of the Vision Transformer model on the 10000 test images: 60.44%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TUWq_H7C6RT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}